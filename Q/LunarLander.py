import gymnasium as gym
import numpy as np
import math
from bayes_opt import BayesianOptimization


# env = gym.make("LunarLander-v2", render_mode="human")
# observation, info = env.reset(seed=42)
# for _ in range(1000):
#    action = env.action_space.sample()  # this is where you would insert your policy
#    observation, reward, terminated, truncated, info = env.step(action)

#    if terminated or truncated:
#       observation, info = env.reset()
# env.close()


def set_up_agent(buckets):
    class LunarLanderAgent():
        
      # Using params optermised on acrobat q learning
      def __init__(self, num_episodes=1000, min_lr=0.001, min_epsilon=0.5, discount=0.9999, decay=42.14476816745908):
         self.buckets = buckets
         self.num_episodes = num_episodes
         self.min_lr = min_lr
         self.min_epsilon = min_epsilon
         self.discount = discount
         self.decay = decay

         self.env = gym.make('LunarLander-v2')

         # [Cosine of theta1, Sine of theta1, Cosine of theta2, Sine of theta2, Angular velocity of theta1, Angular velocity of theta2]
         self.upper_bounds = [1.5, 1.5, 5, 5, self.env.observation_space.high[4], 5, 1, 1]
         self.lower_bounds = [-1.5, -1.5, -5, -5, self.env.observation_space.low[4], -5, 0, 0]

         self.q_learning_table = np.zeros(self.buckets + (self.env.action_space.n,))
            
      def discretize_state(self, obs):
         discretized = list()
         if type(obs) == tuple:
               obs = obs[0]
         for i in range(len(obs)):
               scaling = (obs[i] + abs(self.lower_bounds[i])) / (self.upper_bounds[i] - self.lower_bounds[i])
               new_obs = int(round((self.buckets[i] - 1) * scaling))
               new_obs = min(self.buckets[i] - 1, max(0, new_obs))
               discretized.append(new_obs)
         return tuple(discretized)

      def choose_action(self, state):
         if (np.random.random() < self.epsilon):
               return self.env.action_space.sample() 
         else:
               return np.argmax(self.q_learning_table[state])

      def update_q_learning(self, state, action, reward, next_state):
         """
         Updates the Q-table with the Q-learning algorithm.
         """
         old_q_value = self.q_learning_table[state][action]
         next_max_q_value = np.max(self.q_learning_table[next_state])
         new_q_value = (1 - self.learning_rate) * old_q_value + self.learning_rate * (reward + self.discount * next_max_q_value)
         self.q_learning_table[state][action] = new_q_value

      def get_epsilon(self, t):
         return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))

      def get_learning_rate(self, t):
         return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))

      def train(self):
         for e in range(self.num_episodes):
               current_state = self.discretize_state(self.env.reset())

               if e % 100 == 0:
                   print(f'Episode : {e}')

               self.learning_rate = self.get_learning_rate(e)
               self.epsilon = self.get_epsilon(e)

               t = 0
               done = False
               while not done and t<500:
                  action = self.choose_action(current_state)
                  obs, reward, done, _1, _2 = self.env.step(action)
                  new_state = self.discretize_state(obs)
                  self.update_q_learning(current_state, action, reward, new_state)
                  current_state = new_state
                  t+=1

         print('Finished training!')

      def run(self):
         # self.env = gym.make("LunarLander-v2", render_mode="human")
         self.env = gym.make("LunarLander-v2")
         
         done = False
         current_state = self.discretize_state(self.env.reset())
         total_reward = 0
         t = 0

         while not done and t<500:
                  self.env.render()
                  action = np.argmax(self.q_learning_table[current_state])
                  obs, reward, done, _1, _2 = self.env.step(action)
                  new_state = self.discretize_state(obs)
                  current_state = new_state
                  total_reward += reward
                  t += 1

         self.env.close() 
         return(total_reward)  
    return LunarLanderAgent()

# Test
def test_agent():
   buckets = tuple([3, 1, 1, 5, 6, 9, 2, 2])   
   scores = []
   #test 1 agents
   for i in range(1):
      if __name__ == "__main__":
         agent = set_up_agent(buckets)  
         agent.train()
         
         #test each agent 10 times
         for j in range(10):
               t = agent.run()
               scores.append(t)

         #Get average of the test    
   ave_of_agent = sum(scores)/10
   print(ave_of_agent)
   return scores


# _100_scores = []
# for i in range(10):
#    _100_scores.append(test_agent())
#    print('On run ', i, ' | scores = ', _100_scores)

# print('100 runs = ', _100_scores)

# ave score =  -270.89827384561465

#Bayes opt v1

# pbounds = {'bucket_0': (1, 10), 'bucket_1': (1, 10), 'bucket_2': (1, 10), 'bucket_3': (1, 10), 'bucket_4': (1, 30), 'bucket_5': (1, 30), 'bucket_6': (1, 2), 'bucket_7': (1, 2)}

# optimizer = BayesianOptimization(
#     f=test_agent,
#     pbounds=pbounds,
#     random_state=1,
# )

# optimizer.maximize(
#     init_points=10,
#     n_iter=100,
# )

# print(optimizer.max)

# {'target': -65.59692216276979, 'params': {'bucket_0': 3.0131297027985826, 'bucket_1': 1.1726910371291428, 'bucket_2': 1.4446142992296624, 'bucket_3': 5.032198438972295, 'bucket_4': 6.02178496266225, 'bucket_5': 9.893969986990838, 'bucket_6': 1.8999642546619842, 'bucket_7': 1.2088341188357326}}



runs =  [[-81.59364638524364, -39.303940021866445, -51.39993074195287, -63.611539930688096, -49.58148076958102, -77.20565207750155, -57.964007184142254, -72.05491131420062, -62.407065692993456, -60.69516820775121], [-116.47322105854435, -76.24397352795367, -496.954791112159, -657.1028337297803, -454.4173910403392, -84.35314717912638, -30.02128865047105, -73.47727119368128, -85.57416537283457, -506.62447239414513], [-813.498148007313, -773.9791881587755, -611.7879036850177, -431.42088411042266, -741.8525044564744, -668.1443762376412, -849.3539951718371, -578.3488527389952, -613.133600613032, -788.5728227660395], [-334.16611483512577, -318.86872923450164, -320.12931387322374, -339.84057265421535, -313.19920226081217, -255.3887418627778, -296.1337938770936, -290.17832151041614, -300.56990077212515, -311.3296964800857], [-796.7492157168748, -90.6342704464712, -91.48672710582615, -690.6865134788641, -87.01868068346711, -77.98224412265999, -8.764919027464728, -62.35408315919837, -511.5340266202157, -73.98965516145836], [-543.480256437704, -508.06597594805226, -557.0216995230605, -554.5568227038689, -727.1730179948329, -558.7235736909506, -567.913555589619, -483.19943070141574, -488.4030492459948, -618.2973167547918], [-479.17850052826816, -509.93043255631613, -340.5240267066672, -535.7404623669914, -479.55355599137283, -464.4245013192427, -754.8870882988736, -498.0806069266571, -552.7266333148696, -507.0030463700238], [-66.77103368113991, -56.32017019909868, -142.546228693335, -196.4596378969411, -82.47684537349306, -85.76120111844321, -88.48259434262731, -194.06488928453416, -61.458669558655785, -83.63319785264204], [-579.9795774358713, -813.2717392660055, -934.969113918357, -626.3509308880084, -432.66560323428047, -799.8762398606353, -931.1954734009448, -664.6282132121456, -479.6391847364582, -547.0815806420416], [-305.711276901847, -252.12220532620546, -542.8297245314182, -341.31124945196973, -342.27571567856785, -279.28616138633504, -270.3555896742903, -758.9370622443477, -409.6465496912316, -487.82889474755365]]
test_data_q_leanr_acrobot_params = []
for i in range(10):
    for j in range(10):
        test_data_q_leanr_acrobot_params.append(runs[i][j])

results =  [[-81.71684213809172, -82.31897634846563, -50.02812533398937, 110.46440857424957, -79.13003362128144], [-71.98983805314107, -93.91036193512089, -165.271399015039, -63.56222789164418, -57.417169402357345], [108.57273462212446, 24.43403094935153, 12.107385212859272, 14.182381302290045, 40.08880029843683], [-64.88026098041053, 75.16926102966195, -49.650955186931576, -53.138193272342924, -71.92986385550624], [-22.653065671748724, -80.70134695358875, -76.35339940627838, -75.54414119957805, -48.53679517755472], [-76.72423947962022, -73.56399626094255, -113.74272420995469, -73.92862518446321, -52.15641416303325], [113.07181383566707, 65.01152592185791, -58.41722301282019, -43.21041370059271, -68.89545917175553], [-40.95653661483906, -83.82371297733475, 56.72376546745719, -75.52680299370495, 76.16736095083567], [-70.74827237547827, -36.94658877101773, -93.73030360673158, -78.30681841610448, -110.3759435798237], [-229.0340205436174, -62.26801588674845, -64.33517018491591, 73.16640032804955, 87.58040660759836],[-168.05806229886193, -172.59748231208002, -118.12320149009443, -79.637054808371, -107.25281895077046], [-73.85525675813217, 11.164019738837265, 25.68973834901844, 37.88947534811135, -16.024732412965623], [-47.67137293734262, -51.010652475514114, -60.842675033465305, -122.0393519234932, 52.402278289789116], [-38.09132522907379, -152.41379703834372, -152.53295486764938, -141.15136298048958, -132.85677650432976], [32.1841142798763, 9.64548545421149, 59.454949897256085, -19.195918645616175, 115.92123577300313], [-112.79285065425954, -44.38717453035026, -16.466354017743214, -50.37158677279895, 103.85651554343244], [-189.08312493001029, -94.21066652670413, -333.40786288907464, -143.3522140171935, -94.59851752253871], [-60.634871393050354, 30.785209368402306, -28.966602447467114, 91.14748904211258, -192.48521664711942], [-28.862934097384542, -8.381600276985958, -48.10047490959012, 42.692532111217766, -15.578798837288183], [-57.65680755386738, -88.20198893514205, -0.10608167066504182, 45.23637297316881, 56.775114613741245]]
test_data_ppo_mountaincar_params = []
for i in range(20):
    for j in range(5):
        test_data_ppo_mountaincar_params.append(results[i][j])
        

print('test_data_q_leanr_acrobot_params = ',test_data_q_leanr_acrobot_params)
print('test_data_ppo_mountaincar_params = ',test_data_ppo_mountaincar_params)
# print(test_data_q_leanr_acrobot_params)
# print(len([-81.59364638524364, -39.303940021866445, -51.39993074195287, -63.611539930688096, -49.58148076958102, -77.20565207750155, -57.964007184142254, -72.05491131420062, -62.407065692993456, -60.69516820775121, -116.47322105854435, -76.24397352795367, -496.954791112159, -657.1028337297803, -454.4173910403392, -84.35314717912638, -30.02128865047105, -73.47727119368128, -85.57416537283457, -506.62447239414513, -813.498148007313, -773.9791881587755, -611.7879036850177, -431.42088411042266, -741.8525044564744, -668.1443762376412, -849.3539951718371, -578.3488527389952, -613.133600613032, -788.5728227660395, -334.16611483512577, -318.86872923450164, -320.12931387322374, -339.84057265421535, -313.19920226081217, -255.3887418627778, -296.1337938770936, -290.17832151041614, -300.56990077212515, -311.3296964800857, -796.7492157168748, -90.6342704464712, -91.48672710582615, -690.6865134788641, -87.01868068346711, -77.98224412265999, -8.764919027464728, -62.35408315919837, -511.5340266202157, -73.98965516145836, -543.480256437704, -508.06597594805226, -557.0216995230605, -554.5568227038689, -727.1730179948329, -558.7235736909506, -567.913555589619, -483.19943070141574, -488.4030492459948, -618.2973167547918, -479.17850052826816, -509.93043255631613, -340.5240267066672, -535.7404623669914, -479.55355599137283, -464.4245013192427, -754.8870882988736, -498.0806069266571, -552.7266333148696, -507.0030463700238, -66.77103368113991, -56.32017019909868, -142.546228693335, -196.4596378969411, -82.47684537349306, -85.76120111844321, -88.48259434262731, -194.06488928453416, -61.458669558655785, -83.63319785264204, -579.9795774358713, -813.2717392660055, -934.969113918357, -626.3509308880084, -432.66560323428047, -799.8762398606353, -931.1954734009448, -664.6282132121456, -479.6391847364582, -547.0815806420416, -305.711276901847, -252.12220532620546, -542.8297245314182, -341.31124945196973, -342.27571567856785, -279.28616138633504, -270.3555896742903, -758.9370622443477, -409.6465496912316, -487.82889474755365]))


# LunaLander box plot
import matplotlib.pyplot as plt
import statistics

dqn = [-43.37768872585578, 289.7056577016607, 167.65523660907712, 248.66115165521933, 240.21540271347513, 206.59026187473862, 199.9407948512289, 300.6046455754738, 245.23376274262233, 227.23181622072048, 201.8376527803565, 259.6712621388749, 291.3479230690373, 267.4030357925669, 183.09536031975955, 195.98339020653108, -27.39111823029699, 142.44469685481883, 181.3544069286176, 36.67032699939941, 217.09342289363258, 166.22268659732586, 171.69531762127838, 226.93179915976327, 231.03985421826633, 123.68699229094392, 214.07599258377368, 214.97739966259644, 246.8278487743031, 248.24638789375643, 205.97603456513366, 232.71572909300716, 210.376935702705, 39.192209352712354, 213.46094521079175, 178.4175651977311, 296.29090509010695, 189.60029967971815, 286.53671242892415, 281.8218224566782, 223.9519173974333, 174.33334958743683, 252.5081902322088, 122.03509041054838, -29.572622013744194, 245.06208372700453, 303.8346129197373, 289.69992337649336, 282.569574479588, 305.3922569537725, 198.75708303535737, 228.6504467918354, -21.815809083883053, 245.2467792175563, 274.7268148360497, 264.5311310601061, 215.65465687223576, -23.53683715921855, 266.4815845233378, 131.84486719646026, 112.97762252142873, 278.6345259538259, 239.79677707531405, 277.2669807887869, 291.46762568627935, 109.75262013793935, 277.33722530528837, 263.4475009368997, 253.58026334750784, 226.7309295714383, 221.52844317302856, 286.7448495437635, 261.89006737014125, 205.9773283333133, 235.658127885466, 223.72066411829428, 187.62588651516512, 278.0344083009767, 209.90315066142068, 187.82724139068512, 224.868872946046, 211.28123717407897, 222.2908605183892, 280.17894756324176, 231.84731123551995, 261.2120559207947, 244.6008667404554, 243.0324205444625, -80.20023868576911, 224.0639280633743, 286.34273163975, 231.97190700276212, 277.5122998533882, 287.1471521443124, 258.81748640227545, 249.1044031904369, 212.26275998762316, 280.0657204902899, 212.94731629316888, 248.09588057288286]
stable_baselings_ppo_on_LL = [241.58,253.44,288.36,230.04,256.39,222.73,240.87,224.29,268.17,280.57,231.34,250.17,189.85,247.36,275.81,214.58,233.16,255.45,242.95,254.00,261.64,254.96,266.93,281.28,285.30,226.84,266.31,272.28,240.34,258.10,285.30,235.34,232.38,273.05,255.70,225.43,276.89,236.27,247.69,266.15,233.31,260.51,282.27,248.11,216.02,238.91,204.06,243.25,213.85,246.01,276.90,228.85,233.40,250.13,247.72,218.00,239.06,257.94,277.32,225.04,222.81,193.33,259.37,263.41,251.67,277.93,291.52,249.05,249.25,213.35,222.15,281.92,234.65,267.08,221.86,263.14,265.66,218.79,229.94,269.14,252.96,196.87,239.47,271.94,276.50,274.26,229.37,196.86,253.31,219.39,273.28,215.63,232.70,206.50,266.49,242.31,249.67,40.26,264.18,234.45]
stable_baselings_DQN_on_LL = [120.88,282.38,210.36,163.74,-81.38,135.33,148.58,126.21,243.46,148.08,104.38,141.38,138.23,151.97,270.39,125.08,133.61,23.22,-5.99,154.83,261.08,169.14,248.05,35.36,261.68,111.30,-117.17,7.78,120.88,282.38,210.36,163.74,-81.38,135.33,148.58,126.21,243.46,148.08,104.38,141.38,138.23,151.97,270.39,125.08,133.61,23.22,-5.99,154.83,261.08,169.14,248.05,35.36,261.68,111.30,-117.17,7.78,166.76,157.31,106.62,243.19,146.84,205.52,137.85,273.84,143.25,138.71,130.60,-178.27,12.70,4.93,157.25,216.29,156.29,126.70,120.48,273.06,139.04,162.94,138.99,131.98,111.00,154.87,243.30,168.74,120.53,279.65,129.84,149.19,11.57,14.72,154.53,144.63,258.65,163.29,174.14,250.80,120.88,282.38,210.36,163.74,-81.38,135.33]

data = [test_data_q_leanr_acrobot_params, test_data_ppo_mountaincar_params, dqn, stable_baselings_ppo_on_LL, stable_baselings_DQN_on_LL]

#get medians
med1, med2, med3, med4, med5 = round(statistics.median(test_data_q_leanr_acrobot_params)), round(statistics.median(test_data_ppo_mountaincar_params)), round(statistics.median(dqn)), round(statistics.median(stable_baselings_ppo_on_LL)), round(statistics.median(stable_baselings_DQN_on_LL))

fig1, ax1 = plt.subplots()
# ax1.set_title('Testing on LunarLander')
ax1.boxplot(data)
plt.xticks([1, 2, 3, 4, 5], ['Q-learning tuned on Acrobot', 'PPO tuned on MountainCar', 'Our DQN', 'PPO StableBaslines', 'DQN StableBaslines'])
plt.ylabel('Score')

plt.text(1.3, med1, med1)
plt.text(2.3, med2, med2)
plt.text(3.3, med3, med3)
plt.text(4.3, med4, med4)
plt.text(5.3, med5, med5)

# show plot
plt.show()

